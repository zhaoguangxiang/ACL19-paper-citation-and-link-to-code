


Top-cited open-sourced ACL19 papers:
#### Transformer-XL: Attentive Language Models beyond a Fixed-Length Context (citation 445)

+ 论文链接：https://www.aclweb.org/anthology/P19-1285.pdf

+ 代码链接：https://github.com/kimiyoung/transformer-xl

#### Multi-Task Deep Neural Networks for Natural Language Understanding (citation 237)

+ 论文链接：https://www.aclweb.org/anthology/P19-1441.pdf

+ 代码链接：https://github.com/namisan/mt-dnn

#### Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference (citation 105)

+ 论文链接：https://www.aclweb.org/anthology/P19-1334.pdf

+ 代码链接：https://github.com/tommccoy1/hans

#### ERNIE: Enhanced Language Representation with Informative Entities (citation 92)

+ 论文链接：https://www.aclweb.org/anthology/P19-1139.pdf

+ 代码链接：https://github.com/thunlp/ERNIE

#### Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned (citation 89)

+ 论文链接：https://www.aclweb.org/anthology/P19-1580.pdf

+ 代码链接：https://github.com/lena-voita/the-story-of-heads

#### What Does BERT Learn about the Structure of Language? (citation 76)

+ 论文链接：https://www.aclweb.org/anthology/P19-1356.pdf

+ 代码链接：https://github.com/ganeshjawahar/interpret_bert

#### Is Attention Interpretable? (citation 61)

+ 论文链接：https://www.aclweb.org/anthology/P19-1282.pdf

+ 代码链接：https://github.com/serrano-s/attn-tests


#### How to (Properly) Evaluate Cross-Lingual Word Embeddings: On Strong Baselines, Comparative Analyses, and Some Misconceptions (citation 61)

+ 论文链接：https://www.aclweb.org/anthology/P19-1070.pdf

+ 代码链接：https://github.com/codogogo/xling-eval

#### Probing Neural Network Comprehension of Natural Language Arguments (citation 58)

+ 论文链接：https://www.aclweb.org/anthology/P19-1459.pdf

+ 代码链接：https://github.com/IKMLab/arct2

The rest can be found in [ACL2019 paper, citation and link to code](https://github.com/zhaoguangxiang/NLP-Conferences-Code/blob/master/ACL/2019/ACL2019.md)
